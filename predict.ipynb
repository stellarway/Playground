{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "path = '../preprocess/'\n",
    "path2 = '../raw/'\n",
    "train_preprocess_1 = pd.read_csv(path+'train_preprocess_1.csv', index_col=[0])\n",
    "test1_preprocess_1 = pd.read_csv(path+'test1_preprocess_1.csv', index_col=[0])\n",
    "test2_preprocess_1 = pd.read_csv(path+'test2_preprocess_1.csv', index_col=[0])\n",
    "activity = pd.read_csv(path2 + 'train_activity.csv')\n",
    "label = pd.read_csv(path2 + 'train_label.csv')\n",
    "\n",
    "def lstm(test_set = test1_preprocess_1, predictY = 'survival_time'):\n",
    "\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    import matplotlib\n",
    "    import pandas as pd\n",
    "    import os\n",
    "\n",
    "    tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "    def MinMaxScaler(data):\n",
    "\n",
    "        numerator = data - np.min(data, 0)\n",
    "        denominator = np.max(data, 0) - np.min(data, 0)\n",
    "        # noise term prevents the zero division\n",
    "        return numerator / (denominator + 1e-7)\n",
    "\n",
    "    def normalize(df):\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(df)\n",
    "        df_scaled = scaler.transform(df)\n",
    "        return df_scaled\n",
    "\n",
    "    # train Parameters\n",
    "    seq_length = 28 # 한 유저의 28일간 데이터를 사용함.\n",
    "    data_dim = 33 # acc_id, day를 제외하고 33개의 피쳐를 가짐\n",
    "    hidden_dim = 10 # 사용자 임의\n",
    "    output_dim = 1 # 출력노드는 1\n",
    "    learning_rate = 0.01\n",
    "    iterations = 2000 # 역전파 횟수\n",
    "    \n",
    "    print(1)\n",
    "\n",
    "    # 데이터 로드\n",
    "    df = train_preprocess_1\n",
    "    unique = activity.acc_id.unique()\n",
    "    acc_id_lst = sorted(list(unique)*28)\n",
    "    day_lst = (list(range(1,29))*len(unique))\n",
    "    full_acc_id_day = pd.DataFrame({'acc_id' : acc_id_lst,\n",
    "                                    'day' : day_lst})\n",
    "\n",
    "    \n",
    "    new_label= label.sort_values('acc_id').reset_index(drop=True)\n",
    "    full_label_result = full_acc_id_day.merge(new_label,on=['acc_id'], how = 'left')\n",
    "\n",
    "    label_series =full_label_result[predictY]\n",
    "    df[predictY] = label_series\n",
    "    \n",
    "    print(2)\n",
    "\n",
    "    # train/test split\n",
    "    # train_size = int(len(xy) * 0.7)\n",
    "    xy = df.values[:,2:]\n",
    "    train_set = xy\n",
    "    test_set = test_set.values[:,2:]\n",
    "\n",
    "    # Scale each\n",
    "    train_set[:,:-1] = normalize(train_set[:,:-1])\n",
    "    test_set = normalize(test_set)\n",
    "    \n",
    "    print(3)\n",
    "\n",
    "    # build datasets\n",
    "    def build_dataset(time_series, seq_length):\n",
    "        dataX = []\n",
    "        dataY = []\n",
    "        for i in range(0, len(time_series) - seq_length + 1, seq_length):\n",
    "            _x = time_series[i:i + seq_length, :-1]\n",
    "            _y = time_series[i + seq_length - 1, [-1]]  # Next close price\n",
    "    #         print(_x, \"->\", _y)\n",
    "            dataX.append(_x)\n",
    "            dataY.append(_y)\n",
    "        return np.array(dataX), np.array(dataY)\n",
    "\n",
    "    trainX, trainY = build_dataset(train_set, seq_length)\n",
    "    \n",
    "    print(4)\n",
    "\n",
    "    def build_dataset_for_test(time_series, seq_length):\n",
    "        dataX = []\n",
    "    #     dataY = []\n",
    "        for i in range(0, len(time_series) - seq_length + 1, seq_length):\n",
    "            _x = time_series[i:i + seq_length]\n",
    "    #         _y = time_series[i + seq_length - 1, [-1]]  # Next close price\n",
    "    #         print(_x, \"->\", _y)\n",
    "            dataX.append(_x)\n",
    "    #         dataY.append(_y)\n",
    "        return np.array(dataX)\n",
    "\n",
    "    testX = build_dataset_for_test(test_set, seq_length)\n",
    "    \n",
    "    print(5)\n",
    "\n",
    "    # input place holders\n",
    "    X = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "    Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "    # build a LSTM network\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(\n",
    "        num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh) #reuse = tf.get_variable_scope().reuse\n",
    "    outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "    Y_pred = tf.contrib.layers.fully_connected(\n",
    "        outputs[:, -1], output_dim, activation_fn=None)  # We use the last cell's output\n",
    "\n",
    "    # cost/loss\n",
    "    loss = tf.reduce_sum(tf.square(Y_pred - Y))  # sum of the squares\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train = optimizer.minimize(loss)\n",
    "    \n",
    "    print(6)\n",
    "    \n",
    "\n",
    "\n",
    "    loss_lst = []\n",
    "    with tf.Session() as sess:\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "\n",
    "        # Training step\n",
    "        for i in range(iterations):\n",
    "            _, step_loss = sess.run([train, loss], feed_dict={\n",
    "                                    X: trainX, Y: trainY})\n",
    "            print(\"[step: {}] loss: {}\".format(i, step_loss))\n",
    "            loss_lst.append(step_loss)\n",
    "\n",
    "        # Test step\n",
    "        \n",
    "        test_predict = sess.run(Y_pred, feed_dict={X: testX})\n",
    "        \n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "    plt.plot(loss_lst)\n",
    "    \n",
    "\n",
    "    del df[predictY]\n",
    "    return test_predict\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "with tf.variable_scope(\"lstm\",reuse=tf.AUTO_REUSE) as scope:\n",
    "    test1_survival_time_predict = lstm(test1_preprocess_1, 'survival_time')\n",
    "    \n",
    "    test1_amount_spent_predict = lstm(test1_preprocess_1, 'amount_spent')\n",
    "    test2_survival_time_predict = lstm(test2_preprocess_1, 'survival_time')\n",
    "    test2_amount_spent_predict = lstm(test2_preprocess_1, 'amount_spent')\n",
    "\n",
    "test1_label = pd.DataFrame({'acc_id' : test1_preprocess_1.acc_id.unique(),\n",
    "              'survival_time' : test1_survival_time_predict.reshape(-1),\n",
    "              'amount_spent' : test1_amount_spent_predict.reshape(-1)})\n",
    "\n",
    "test2_label = pd.DataFrame({'acc_id' : test2_preprocess_1.acc_id.unique(),\n",
    "              'survival_time' : test2_survival_time_predict.reshape(-1),\n",
    "              'amount_spent' : test2_amount_spent_predict.reshape(-1)})\n",
    "\n",
    "test1_label.to_csv('test1_label.csv', encoding = 'utf8')\n",
    "test2_label.to_csv('test2_label.csv', encoding = 'utf8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
